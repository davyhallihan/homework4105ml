{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8xi_SPH2Wkt"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.modules.loss import L1Loss\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsnsifS0NYCE",
        "outputId": "deb8caef-def9-4bc7-9821-f1e496bcb6d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gP3vgW4RNpH8"
      },
      "outputs": [],
      "source": [
        "# Read Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/4105av/fulloutput.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split truth off and form dfX\n",
        "dfT = df['malware'].values\n",
        "dfX = df.drop(['malware', 'length', 'id'], axis=1)"
      ],
      "metadata": {
        "id": "ePg4E_0A_pZ-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classical\n",
        "\n",
        "#Train Test\n",
        "np.random.seed(0)\n",
        "Xtrain, Xval, Ttrain, Tval = train_test_split(dfX, dfT, train_size = 0.8, test_size=0.2)\n",
        "\n",
        "model = LogisticRegression(C=0.01, max_iter = 100000) #With reg.\n",
        "model.fit(Xtrain, Ttrain)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "predictions = model.predict(Xval)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(Tval, predictions)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(Tval, predictions)\n",
        "precision = precision_score(Tval, predictions)\n",
        "recall = recall_score(Tval, predictions)\n",
        "f1 = f1_score(Tval, predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5wTFG4AAqN9",
        "outputId": "6224961e-a96a-4d6a-a257-fb6fe821fd2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[14479  2807]\n",
            " [ 2491 20533]]\n",
            "Accuracy: 0.8686\n",
            "Precision: 0.8797\n",
            "Recall: 0.8918\n",
            "F1 Score: 0.8857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oftg1KJu8JGQ",
        "outputId": "79c5b640-aa78-4d1e-9a12-749fd3a0a01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Epoch 0, Loss 0.793873, Validation Loss 0.802645\n",
            "Epoch 1, Loss 0.754302, Validation Loss 0.762868\n",
            "Epoch 2, Loss 0.722944, Validation Loss 0.730986\n",
            "Epoch 3, Loss 0.697540, Validation Loss 0.705247\n",
            "Epoch 4, Loss 0.673393, Validation Loss 0.680770\n",
            "Epoch 5, Loss 0.648281, Validation Loss 0.655080\n",
            "Epoch 6, Loss 0.621885, Validation Loss 0.628281\n",
            "Epoch 7, Loss 0.594517, Validation Loss 0.600979\n",
            "Epoch 8, Loss 0.565983, Validation Loss 0.572018\n",
            "Epoch 9, Loss 0.537070, Validation Loss 0.542471\n",
            "Epoch 10, Loss 0.507491, Validation Loss 0.513110\n",
            "Epoch 11, Loss 0.477911, Validation Loss 0.482800\n",
            "Epoch 12, Loss 0.448908, Validation Loss 0.452616\n",
            "Epoch 13, Loss 0.421386, Validation Loss 0.424519\n",
            "Epoch 14, Loss 0.395599, Validation Loss 0.399141\n",
            "Epoch 15, Loss 0.373410, Validation Loss 0.375357\n",
            "Epoch 16, Loss 0.355818, Validation Loss 0.356291\n",
            "Epoch 17, Loss 0.342282, Validation Loss 0.343722\n",
            "Epoch 18, Loss 0.336044, Validation Loss 0.336684\n",
            "Epoch 19, Loss 0.335922, Validation Loss 0.334033\n",
            "Epoch 20, Loss 0.340522, Validation Loss 0.338775\n",
            "Epoch 21, Loss 0.345251, Validation Loss 0.345289\n",
            "Epoch 22, Loss 0.348706, Validation Loss 0.349426\n",
            "Epoch 23, Loss 0.348807, Validation Loss 0.346750\n",
            "Epoch 24, Loss 0.344902, Validation Loss 0.341316\n",
            "Epoch 25, Loss 0.336706, Validation Loss 0.334177\n",
            "Epoch 26, Loss 0.326909, Validation Loss 0.327148\n",
            "Epoch 27, Loss 0.317456, Validation Loss 0.316395\n",
            "Epoch 28, Loss 0.308743, Validation Loss 0.307499\n",
            "Epoch 29, Loss 0.300701, Validation Loss 0.300609\n",
            "Epoch 30, Loss 0.293874, Validation Loss 0.293714\n",
            "Epoch 31, Loss 0.289117, Validation Loss 0.288132\n",
            "Epoch 32, Loss 0.284296, Validation Loss 0.285003\n",
            "Epoch 33, Loss 0.280869, Validation Loss 0.281765\n",
            "Epoch 34, Loss 0.277355, Validation Loss 0.278237\n",
            "Epoch 35, Loss 0.273131, Validation Loss 0.273139\n",
            "Epoch 36, Loss 0.269581, Validation Loss 0.270515\n",
            "Epoch 37, Loss 0.264387, Validation Loss 0.265213\n",
            "Epoch 38, Loss 0.259282, Validation Loss 0.260031\n",
            "Epoch 39, Loss 0.253101, Validation Loss 0.253344\n",
            "Epoch 40, Loss 0.247945, Validation Loss 0.248718\n",
            "Epoch 41, Loss 0.242429, Validation Loss 0.242627\n",
            "Epoch 42, Loss 0.237088, Validation Loss 0.237382\n",
            "Epoch 43, Loss 0.232571, Validation Loss 0.233659\n",
            "Epoch 44, Loss 0.228284, Validation Loss 0.228995\n",
            "Epoch 45, Loss 0.224356, Validation Loss 0.224688\n",
            "Epoch 46, Loss 0.221326, Validation Loss 0.221053\n",
            "Epoch 47, Loss 0.217759, Validation Loss 0.217864\n",
            "Epoch 48, Loss 0.213675, Validation Loss 0.214001\n",
            "Epoch 49, Loss 0.209596, Validation Loss 0.210218\n",
            "Epoch 50, Loss 0.204867, Validation Loss 0.204396\n",
            "Epoch 51, Loss 0.200892, Validation Loss 0.201398\n",
            "Epoch 52, Loss 0.196130, Validation Loss 0.196517\n",
            "Epoch 53, Loss 0.192782, Validation Loss 0.193437\n",
            "Epoch 54, Loss 0.189761, Validation Loss 0.189759\n",
            "Epoch 55, Loss 0.186289, Validation Loss 0.186651\n",
            "Epoch 56, Loss 0.183700, Validation Loss 0.184659\n",
            "Epoch 57, Loss 0.180948, Validation Loss 0.182041\n",
            "Epoch 58, Loss 0.178356, Validation Loss 0.179138\n",
            "Epoch 59, Loss 0.175563, Validation Loss 0.177190\n",
            "Epoch 60, Loss 0.172984, Validation Loss 0.174196\n",
            "Epoch 61, Loss 0.171226, Validation Loss 0.171138\n",
            "Epoch 62, Loss 0.168456, Validation Loss 0.169778\n",
            "Epoch 63, Loss 0.166889, Validation Loss 0.167966\n",
            "Epoch 64, Loss 0.164831, Validation Loss 0.165971\n",
            "Epoch 65, Loss 0.163346, Validation Loss 0.163936\n",
            "Epoch 66, Loss 0.161584, Validation Loss 0.163031\n",
            "Epoch 67, Loss 0.159237, Validation Loss 0.160765\n",
            "Epoch 68, Loss 0.157593, Validation Loss 0.158802\n",
            "Epoch 69, Loss 0.155669, Validation Loss 0.157607\n",
            "Epoch 70, Loss 0.153962, Validation Loss 0.156141\n",
            "Epoch 71, Loss 0.152561, Validation Loss 0.153738\n",
            "Epoch 72, Loss 0.150734, Validation Loss 0.151904\n",
            "Epoch 73, Loss 0.149390, Validation Loss 0.150692\n",
            "Epoch 74, Loss 0.147666, Validation Loss 0.149227\n",
            "Epoch 75, Loss 0.146926, Validation Loss 0.147870\n",
            "Epoch 76, Loss 0.145317, Validation Loss 0.146117\n",
            "Epoch 77, Loss 0.144220, Validation Loss 0.145986\n",
            "Epoch 78, Loss 0.142923, Validation Loss 0.144392\n",
            "Epoch 79, Loss 0.141509, Validation Loss 0.142872\n",
            "Epoch 80, Loss 0.140474, Validation Loss 0.142239\n",
            "Epoch 81, Loss 0.139309, Validation Loss 0.141161\n",
            "Epoch 82, Loss 0.138073, Validation Loss 0.139855\n",
            "Epoch 83, Loss 0.137596, Validation Loss 0.138692\n",
            "Epoch 84, Loss 0.136371, Validation Loss 0.137653\n",
            "Epoch 85, Loss 0.135385, Validation Loss 0.136903\n",
            "Epoch 86, Loss 0.134793, Validation Loss 0.136589\n",
            "Epoch 87, Loss 0.133825, Validation Loss 0.135273\n",
            "Epoch 88, Loss 0.133052, Validation Loss 0.134322\n",
            "Epoch 89, Loss 0.132703, Validation Loss 0.133902\n",
            "Epoch 90, Loss 0.131740, Validation Loss 0.133292\n",
            "Epoch 91, Loss 0.131265, Validation Loss 0.132382\n",
            "Epoch 92, Loss 0.130051, Validation Loss 0.132157\n",
            "Epoch 93, Loss 0.129743, Validation Loss 0.131119\n",
            "Epoch 94, Loss 0.129431, Validation Loss 0.130579\n",
            "Epoch 95, Loss 0.128570, Validation Loss 0.129724\n",
            "Epoch 96, Loss 0.127736, Validation Loss 0.129224\n",
            "Epoch 97, Loss 0.127343, Validation Loss 0.128337\n",
            "Epoch 98, Loss 0.126799, Validation Loss 0.127453\n",
            "Epoch 99, Loss 0.125818, Validation Loss 0.127810\n",
            "Epoch 100, Loss 0.125491, Validation Loss 0.127187\n",
            "Epoch 101, Loss 0.124987, Validation Loss 0.126022\n",
            "Epoch 102, Loss 0.124259, Validation Loss 0.125594\n",
            "Epoch 103, Loss 0.123927, Validation Loss 0.125354\n",
            "Epoch 104, Loss 0.123364, Validation Loss 0.124337\n",
            "Epoch 105, Loss 0.122875, Validation Loss 0.123700\n",
            "Epoch 106, Loss 0.121775, Validation Loss 0.123151\n",
            "Epoch 107, Loss 0.121937, Validation Loss 0.123640\n",
            "Epoch 108, Loss 0.121344, Validation Loss 0.123096\n",
            "Epoch 109, Loss 0.120986, Validation Loss 0.122077\n",
            "Epoch 110, Loss 0.120502, Validation Loss 0.121921\n",
            "Epoch 111, Loss 0.120186, Validation Loss 0.121872\n",
            "Epoch 112, Loss 0.119184, Validation Loss 0.121225\n",
            "Epoch 113, Loss 0.118864, Validation Loss 0.119982\n",
            "Epoch 114, Loss 0.118879, Validation Loss 0.119388\n",
            "Epoch 115, Loss 0.118045, Validation Loss 0.119753\n",
            "Epoch 116, Loss 0.117693, Validation Loss 0.119271\n",
            "Epoch 117, Loss 0.117450, Validation Loss 0.118487\n",
            "Epoch 118, Loss 0.116844, Validation Loss 0.118746\n",
            "Epoch 119, Loss 0.116363, Validation Loss 0.118117\n",
            "Epoch 120, Loss 0.116390, Validation Loss 0.117168\n",
            "Epoch 121, Loss 0.115352, Validation Loss 0.117377\n",
            "Epoch 122, Loss 0.115278, Validation Loss 0.116624\n",
            "Epoch 123, Loss 0.114838, Validation Loss 0.116707\n",
            "Epoch 124, Loss 0.114457, Validation Loss 0.115005\n",
            "Epoch 125, Loss 0.114096, Validation Loss 0.115890\n",
            "Epoch 126, Loss 0.114014, Validation Loss 0.114396\n",
            "Epoch 127, Loss 0.113226, Validation Loss 0.114592\n",
            "Epoch 128, Loss 0.113302, Validation Loss 0.114068\n",
            "Epoch 129, Loss 0.112489, Validation Loss 0.113864\n",
            "Epoch 130, Loss 0.112294, Validation Loss 0.113820\n",
            "Epoch 131, Loss 0.111672, Validation Loss 0.113149\n",
            "Epoch 132, Loss 0.111784, Validation Loss 0.112588\n",
            "Epoch 133, Loss 0.111153, Validation Loss 0.112473\n",
            "Epoch 134, Loss 0.110954, Validation Loss 0.111787\n",
            "Epoch 135, Loss 0.110609, Validation Loss 0.111918\n",
            "Epoch 136, Loss 0.110435, Validation Loss 0.110866\n",
            "Epoch 137, Loss 0.109805, Validation Loss 0.111197\n",
            "Epoch 138, Loss 0.109806, Validation Loss 0.110375\n",
            "Epoch 139, Loss 0.109049, Validation Loss 0.110544\n",
            "Epoch 140, Loss 0.109303, Validation Loss 0.110090\n",
            "Epoch 141, Loss 0.108494, Validation Loss 0.109893\n",
            "Epoch 142, Loss 0.108042, Validation Loss 0.109569\n",
            "Epoch 143, Loss 0.107936, Validation Loss 0.109638\n",
            "Epoch 144, Loss 0.107411, Validation Loss 0.109249\n",
            "Epoch 145, Loss 0.107294, Validation Loss 0.108512\n",
            "Epoch 146, Loss 0.106837, Validation Loss 0.108040\n",
            "Epoch 147, Loss 0.106635, Validation Loss 0.107849\n",
            "Epoch 148, Loss 0.106088, Validation Loss 0.107207\n",
            "Epoch 149, Loss 0.105889, Validation Loss 0.106911\n",
            "Epoch 150, Loss 0.105748, Validation Loss 0.106538\n",
            "Epoch 151, Loss 0.105136, Validation Loss 0.106574\n",
            "Epoch 152, Loss 0.104877, Validation Loss 0.106288\n",
            "Epoch 153, Loss 0.104714, Validation Loss 0.106307\n",
            "Epoch 154, Loss 0.104248, Validation Loss 0.105629\n",
            "Epoch 155, Loss 0.103866, Validation Loss 0.105779\n",
            "Epoch 156, Loss 0.103574, Validation Loss 0.104543\n",
            "Epoch 157, Loss 0.102905, Validation Loss 0.104478\n",
            "Epoch 158, Loss 0.103057, Validation Loss 0.103802\n",
            "Epoch 159, Loss 0.102789, Validation Loss 0.103799\n",
            "Epoch 160, Loss 0.102419, Validation Loss 0.103589\n",
            "Epoch 161, Loss 0.102132, Validation Loss 0.103307\n",
            "Epoch 162, Loss 0.101601, Validation Loss 0.102712\n",
            "Epoch 163, Loss 0.101508, Validation Loss 0.102591\n",
            "Epoch 164, Loss 0.101278, Validation Loss 0.102292\n",
            "Epoch 165, Loss 0.100872, Validation Loss 0.102287\n",
            "Epoch 166, Loss 0.100788, Validation Loss 0.101590\n",
            "Epoch 167, Loss 0.099933, Validation Loss 0.101518\n",
            "Epoch 168, Loss 0.099483, Validation Loss 0.100976\n",
            "Epoch 169, Loss 0.099474, Validation Loss 0.100788\n",
            "Epoch 170, Loss 0.099081, Validation Loss 0.101233\n",
            "Epoch 171, Loss 0.099072, Validation Loss 0.100456\n",
            "Epoch 172, Loss 0.098360, Validation Loss 0.100649\n",
            "Epoch 173, Loss 0.098449, Validation Loss 0.099068\n",
            "Epoch 174, Loss 0.098198, Validation Loss 0.099408\n",
            "Epoch 175, Loss 0.097812, Validation Loss 0.099079\n",
            "Epoch 176, Loss 0.097282, Validation Loss 0.098843\n",
            "Epoch 177, Loss 0.097012, Validation Loss 0.098968\n",
            "Epoch 178, Loss 0.096757, Validation Loss 0.098170\n",
            "Epoch 179, Loss 0.096609, Validation Loss 0.098345\n",
            "Epoch 180, Loss 0.096386, Validation Loss 0.098022\n",
            "Epoch 181, Loss 0.095966, Validation Loss 0.097102\n",
            "Epoch 182, Loss 0.095975, Validation Loss 0.097166\n",
            "Epoch 183, Loss 0.095351, Validation Loss 0.097131\n",
            "Epoch 184, Loss 0.095253, Validation Loss 0.096290\n",
            "Epoch 185, Loss 0.094800, Validation Loss 0.096565\n",
            "Epoch 186, Loss 0.094634, Validation Loss 0.096248\n",
            "Epoch 187, Loss 0.094377, Validation Loss 0.095872\n",
            "Epoch 188, Loss 0.094091, Validation Loss 0.095620\n",
            "Epoch 189, Loss 0.093814, Validation Loss 0.095397\n",
            "Epoch 190, Loss 0.093748, Validation Loss 0.094798\n",
            "Epoch 191, Loss 0.093501, Validation Loss 0.095244\n",
            "Epoch 192, Loss 0.093139, Validation Loss 0.094670\n",
            "Epoch 193, Loss 0.092928, Validation Loss 0.093895\n",
            "Epoch 194, Loss 0.092966, Validation Loss 0.093674\n",
            "Epoch 195, Loss 0.092348, Validation Loss 0.093758\n",
            "Epoch 196, Loss 0.092137, Validation Loss 0.092547\n",
            "Epoch 197, Loss 0.091635, Validation Loss 0.093496\n",
            "Epoch 198, Loss 0.091937, Validation Loss 0.093321\n",
            "Epoch 199, Loss 0.091482, Validation Loss 0.093044\n",
            "Epoch 200, Loss 0.091329, Validation Loss 0.092576\n",
            "Epoch 201, Loss 0.090640, Validation Loss 0.091686\n",
            "Epoch 202, Loss 0.090686, Validation Loss 0.091656\n",
            "Epoch 203, Loss 0.090222, Validation Loss 0.091950\n",
            "Epoch 204, Loss 0.089899, Validation Loss 0.092158\n",
            "Epoch 205, Loss 0.089788, Validation Loss 0.091553\n",
            "Epoch 206, Loss 0.089907, Validation Loss 0.090744\n",
            "Epoch 207, Loss 0.089579, Validation Loss 0.090887\n",
            "Epoch 208, Loss 0.089198, Validation Loss 0.091291\n",
            "Epoch 209, Loss 0.088827, Validation Loss 0.090948\n",
            "Epoch 210, Loss 0.088905, Validation Loss 0.090522\n",
            "Epoch 211, Loss 0.088516, Validation Loss 0.090319\n",
            "Epoch 212, Loss 0.088185, Validation Loss 0.090191\n",
            "Epoch 213, Loss 0.088119, Validation Loss 0.090495\n",
            "Epoch 214, Loss 0.087978, Validation Loss 0.089470\n",
            "Epoch 215, Loss 0.087646, Validation Loss 0.089506\n",
            "Epoch 216, Loss 0.087558, Validation Loss 0.088948\n",
            "Epoch 217, Loss 0.087664, Validation Loss 0.088792\n",
            "Epoch 218, Loss 0.087134, Validation Loss 0.088771\n",
            "Epoch 219, Loss 0.086877, Validation Loss 0.088562\n",
            "Epoch 220, Loss 0.086436, Validation Loss 0.087939\n",
            "Epoch 221, Loss 0.086481, Validation Loss 0.088354\n",
            "Epoch 222, Loss 0.086315, Validation Loss 0.087542\n",
            "Epoch 223, Loss 0.086261, Validation Loss 0.088017\n",
            "Epoch 224, Loss 0.086181, Validation Loss 0.087310\n",
            "Epoch 225, Loss 0.085908, Validation Loss 0.087047\n",
            "Epoch 226, Loss 0.085647, Validation Loss 0.086431\n",
            "Epoch 227, Loss 0.085631, Validation Loss 0.087311\n",
            "Epoch 228, Loss 0.085714, Validation Loss 0.086562\n",
            "Epoch 229, Loss 0.085099, Validation Loss 0.086625\n",
            "Epoch 230, Loss 0.084969, Validation Loss 0.086353\n",
            "Epoch 231, Loss 0.084911, Validation Loss 0.086820\n",
            "Epoch 232, Loss 0.084507, Validation Loss 0.086002\n",
            "Epoch 233, Loss 0.084367, Validation Loss 0.086175\n",
            "Epoch 234, Loss 0.084254, Validation Loss 0.086009\n",
            "Epoch 235, Loss 0.084101, Validation Loss 0.085970\n",
            "Epoch 236, Loss 0.083718, Validation Loss 0.085260\n",
            "Epoch 237, Loss 0.083700, Validation Loss 0.085571\n",
            "Epoch 238, Loss 0.083802, Validation Loss 0.085615\n",
            "Epoch 239, Loss 0.083313, Validation Loss 0.084913\n",
            "Epoch 240, Loss 0.083281, Validation Loss 0.084656\n",
            "Epoch 241, Loss 0.083066, Validation Loss 0.085195\n",
            "Epoch 242, Loss 0.083083, Validation Loss 0.085025\n",
            "Epoch 243, Loss 0.082873, Validation Loss 0.084225\n",
            "Epoch 244, Loss 0.082704, Validation Loss 0.085395\n",
            "Epoch 245, Loss 0.082603, Validation Loss 0.083886\n",
            "Epoch 246, Loss 0.082238, Validation Loss 0.084046\n",
            "Epoch 247, Loss 0.082247, Validation Loss 0.084003\n",
            "Epoch 248, Loss 0.082125, Validation Loss 0.083869\n",
            "Epoch 249, Loss 0.081832, Validation Loss 0.084253\n",
            "Epoch 250, Loss 0.081985, Validation Loss 0.083525\n",
            "Epoch 251, Loss 0.081891, Validation Loss 0.082866\n",
            "Epoch 252, Loss 0.081358, Validation Loss 0.083298\n",
            "Epoch 253, Loss 0.081504, Validation Loss 0.083065\n",
            "Epoch 254, Loss 0.081222, Validation Loss 0.082293\n",
            "Epoch 255, Loss 0.081199, Validation Loss 0.082918\n",
            "Epoch 256, Loss 0.081057, Validation Loss 0.083133\n",
            "Epoch 257, Loss 0.081010, Validation Loss 0.082445\n",
            "Epoch 258, Loss 0.080843, Validation Loss 0.082016\n",
            "Epoch 259, Loss 0.080772, Validation Loss 0.082101\n",
            "Epoch 260, Loss 0.080370, Validation Loss 0.081801\n",
            "Epoch 261, Loss 0.080160, Validation Loss 0.081713\n",
            "Epoch 262, Loss 0.080413, Validation Loss 0.082235\n",
            "Epoch 263, Loss 0.080027, Validation Loss 0.081862\n",
            "Epoch 264, Loss 0.080115, Validation Loss 0.081932\n",
            "Epoch 265, Loss 0.079583, Validation Loss 0.081421\n",
            "Epoch 266, Loss 0.080004, Validation Loss 0.081106\n",
            "Epoch 267, Loss 0.079776, Validation Loss 0.081332\n",
            "Epoch 268, Loss 0.079414, Validation Loss 0.081700\n",
            "Epoch 269, Loss 0.079458, Validation Loss 0.081143\n",
            "Epoch 270, Loss 0.079470, Validation Loss 0.080884\n",
            "Epoch 271, Loss 0.079243, Validation Loss 0.080815\n",
            "Epoch 272, Loss 0.079143, Validation Loss 0.080204\n",
            "Epoch 273, Loss 0.078728, Validation Loss 0.079999\n",
            "Epoch 274, Loss 0.078801, Validation Loss 0.081382\n",
            "Epoch 275, Loss 0.078789, Validation Loss 0.080476\n",
            "Epoch 276, Loss 0.078582, Validation Loss 0.080438\n",
            "Epoch 277, Loss 0.078487, Validation Loss 0.079889\n",
            "Epoch 278, Loss 0.078338, Validation Loss 0.079659\n",
            "Epoch 279, Loss 0.078268, Validation Loss 0.079704\n",
            "Epoch 280, Loss 0.078059, Validation Loss 0.079332\n",
            "Epoch 281, Loss 0.078006, Validation Loss 0.079898\n",
            "Epoch 282, Loss 0.077995, Validation Loss 0.079443\n",
            "Epoch 283, Loss 0.077777, Validation Loss 0.079303\n",
            "Epoch 284, Loss 0.077478, Validation Loss 0.079047\n",
            "Epoch 285, Loss 0.077599, Validation Loss 0.079071\n",
            "Epoch 286, Loss 0.077196, Validation Loss 0.079133\n",
            "Epoch 287, Loss 0.077210, Validation Loss 0.079389\n",
            "Epoch 288, Loss 0.077326, Validation Loss 0.078372\n",
            "Epoch 289, Loss 0.077087, Validation Loss 0.078657\n",
            "Epoch 290, Loss 0.077140, Validation Loss 0.078948\n",
            "Epoch 291, Loss 0.077080, Validation Loss 0.078898\n",
            "Epoch 292, Loss 0.076964, Validation Loss 0.079155\n",
            "Epoch 293, Loss 0.076846, Validation Loss 0.078425\n",
            "Epoch 294, Loss 0.076849, Validation Loss 0.078236\n",
            "Epoch 295, Loss 0.076746, Validation Loss 0.078569\n",
            "Epoch 296, Loss 0.076249, Validation Loss 0.078111\n",
            "Epoch 297, Loss 0.076728, Validation Loss 0.078426\n",
            "Epoch 298, Loss 0.076193, Validation Loss 0.078211\n",
            "Epoch 299, Loss 0.076149, Validation Loss 0.077855\n",
            "Epoch 300, Loss 0.076025, Validation Loss 0.077886\n",
            "Epoch 301, Loss 0.075972, Validation Loss 0.078101\n",
            "Epoch 302, Loss 0.076100, Validation Loss 0.078042\n",
            "Epoch 303, Loss 0.075734, Validation Loss 0.077148\n",
            "Epoch 304, Loss 0.075662, Validation Loss 0.076683\n",
            "Epoch 305, Loss 0.075211, Validation Loss 0.077085\n",
            "Epoch 306, Loss 0.075535, Validation Loss 0.077429\n",
            "Epoch 307, Loss 0.075412, Validation Loss 0.077254\n",
            "Epoch 308, Loss 0.075380, Validation Loss 0.077194\n",
            "Epoch 309, Loss 0.075171, Validation Loss 0.076506\n",
            "Epoch 310, Loss 0.074927, Validation Loss 0.077215\n",
            "Epoch 311, Loss 0.074956, Validation Loss 0.077098\n",
            "Epoch 312, Loss 0.074937, Validation Loss 0.076826\n",
            "Epoch 313, Loss 0.074847, Validation Loss 0.076789\n",
            "Epoch 314, Loss 0.074799, Validation Loss 0.076493\n",
            "Epoch 315, Loss 0.074633, Validation Loss 0.076105\n",
            "Epoch 316, Loss 0.074494, Validation Loss 0.076227\n",
            "Epoch 317, Loss 0.074508, Validation Loss 0.076206\n",
            "Epoch 318, Loss 0.074434, Validation Loss 0.075963\n",
            "Epoch 319, Loss 0.074163, Validation Loss 0.076228\n",
            "Epoch 320, Loss 0.074463, Validation Loss 0.076431\n",
            "Epoch 321, Loss 0.073773, Validation Loss 0.076102\n",
            "Epoch 322, Loss 0.073947, Validation Loss 0.075732\n",
            "Epoch 323, Loss 0.073750, Validation Loss 0.075722\n",
            "Epoch 324, Loss 0.073822, Validation Loss 0.076328\n",
            "Epoch 325, Loss 0.073719, Validation Loss 0.075424\n",
            "Epoch 326, Loss 0.073664, Validation Loss 0.076046\n",
            "Epoch 327, Loss 0.073875, Validation Loss 0.075362\n",
            "Epoch 328, Loss 0.073418, Validation Loss 0.075941\n",
            "Epoch 329, Loss 0.073615, Validation Loss 0.074837\n",
            "Epoch 330, Loss 0.072906, Validation Loss 0.074499\n",
            "Epoch 331, Loss 0.073077, Validation Loss 0.075060\n",
            "Epoch 332, Loss 0.073067, Validation Loss 0.074485\n",
            "Epoch 333, Loss 0.073093, Validation Loss 0.075044\n",
            "Epoch 334, Loss 0.073338, Validation Loss 0.075057\n",
            "Epoch 335, Loss 0.072872, Validation Loss 0.074834\n",
            "Epoch 336, Loss 0.072930, Validation Loss 0.074409\n",
            "Epoch 337, Loss 0.072466, Validation Loss 0.074704\n",
            "Epoch 338, Loss 0.072473, Validation Loss 0.074143\n",
            "Epoch 339, Loss 0.072717, Validation Loss 0.074672\n",
            "Epoch 340, Loss 0.072578, Validation Loss 0.074466\n",
            "Epoch 341, Loss 0.072400, Validation Loss 0.074027\n",
            "Epoch 342, Loss 0.072279, Validation Loss 0.074137\n",
            "Epoch 343, Loss 0.072185, Validation Loss 0.073872\n",
            "Epoch 344, Loss 0.072042, Validation Loss 0.073910\n",
            "Epoch 345, Loss 0.071907, Validation Loss 0.073998\n",
            "Epoch 346, Loss 0.071946, Validation Loss 0.073197\n",
            "Epoch 347, Loss 0.071650, Validation Loss 0.073222\n",
            "Epoch 348, Loss 0.071765, Validation Loss 0.073871\n",
            "Epoch 349, Loss 0.071944, Validation Loss 0.073826\n",
            "Epoch 350, Loss 0.071672, Validation Loss 0.073122\n",
            "Epoch 351, Loss 0.071335, Validation Loss 0.073084\n",
            "Epoch 352, Loss 0.071200, Validation Loss 0.073384\n",
            "Epoch 353, Loss 0.071363, Validation Loss 0.073418\n",
            "Epoch 354, Loss 0.071298, Validation Loss 0.073864\n",
            "Epoch 355, Loss 0.071350, Validation Loss 0.072889\n",
            "Epoch 356, Loss 0.071021, Validation Loss 0.072921\n",
            "Epoch 357, Loss 0.071034, Validation Loss 0.073039\n",
            "Epoch 358, Loss 0.070864, Validation Loss 0.073502\n",
            "Epoch 359, Loss 0.070865, Validation Loss 0.072967\n",
            "Epoch 360, Loss 0.070746, Validation Loss 0.072902\n",
            "Epoch 361, Loss 0.070866, Validation Loss 0.072832\n",
            "Epoch 362, Loss 0.070509, Validation Loss 0.072893\n",
            "Epoch 363, Loss 0.070562, Validation Loss 0.072380\n",
            "Epoch 364, Loss 0.070444, Validation Loss 0.072192\n",
            "Epoch 365, Loss 0.070508, Validation Loss 0.072582\n",
            "Epoch 366, Loss 0.070274, Validation Loss 0.072663\n",
            "Epoch 367, Loss 0.070257, Validation Loss 0.072269\n",
            "Epoch 368, Loss 0.070017, Validation Loss 0.072471\n",
            "Epoch 369, Loss 0.069634, Validation Loss 0.072065\n",
            "Epoch 370, Loss 0.069872, Validation Loss 0.072467\n",
            "Epoch 371, Loss 0.069672, Validation Loss 0.071571\n",
            "Epoch 372, Loss 0.069639, Validation Loss 0.072159\n",
            "Epoch 373, Loss 0.069639, Validation Loss 0.072179\n",
            "Epoch 374, Loss 0.069824, Validation Loss 0.071921\n",
            "Epoch 375, Loss 0.069915, Validation Loss 0.071334\n",
            "Epoch 376, Loss 0.069613, Validation Loss 0.071402\n",
            "Epoch 377, Loss 0.069465, Validation Loss 0.070930\n",
            "Epoch 378, Loss 0.069196, Validation Loss 0.071670\n",
            "Epoch 379, Loss 0.069307, Validation Loss 0.072002\n",
            "Epoch 380, Loss 0.069364, Validation Loss 0.070912\n",
            "Epoch 381, Loss 0.069089, Validation Loss 0.070907\n",
            "Epoch 382, Loss 0.069236, Validation Loss 0.070829\n",
            "Epoch 383, Loss 0.068780, Validation Loss 0.070849\n",
            "Epoch 384, Loss 0.068902, Validation Loss 0.071096\n",
            "Epoch 385, Loss 0.068870, Validation Loss 0.070936\n",
            "Epoch 386, Loss 0.068683, Validation Loss 0.071241\n",
            "Epoch 387, Loss 0.068687, Validation Loss 0.070892\n",
            "Epoch 388, Loss 0.068521, Validation Loss 0.070711\n",
            "Epoch 389, Loss 0.068478, Validation Loss 0.071055\n",
            "Epoch 390, Loss 0.068607, Validation Loss 0.070672\n",
            "Epoch 391, Loss 0.068367, Validation Loss 0.070083\n",
            "Epoch 392, Loss 0.068286, Validation Loss 0.070143\n",
            "Epoch 393, Loss 0.068193, Validation Loss 0.069964\n",
            "Epoch 394, Loss 0.068016, Validation Loss 0.070287\n",
            "Epoch 395, Loss 0.068214, Validation Loss 0.069992\n",
            "Epoch 396, Loss 0.068032, Validation Loss 0.070373\n",
            "Epoch 397, Loss 0.067898, Validation Loss 0.069881\n",
            "Epoch 398, Loss 0.067866, Validation Loss 0.069858\n",
            "Epoch 399, Loss 0.067778, Validation Loss 0.069950\n",
            "Epoch 400, Loss 0.067720, Validation Loss 0.070396\n",
            "Epoch 401, Loss 0.067669, Validation Loss 0.069381\n",
            "Epoch 402, Loss 0.067701, Validation Loss 0.069881\n",
            "Epoch 403, Loss 0.067216, Validation Loss 0.070148\n",
            "Epoch 404, Loss 0.067198, Validation Loss 0.069395\n",
            "Epoch 405, Loss 0.067332, Validation Loss 0.069767\n",
            "Epoch 406, Loss 0.067328, Validation Loss 0.069566\n",
            "Epoch 407, Loss 0.066911, Validation Loss 0.069343\n",
            "Epoch 408, Loss 0.066888, Validation Loss 0.069340\n",
            "Epoch 409, Loss 0.066867, Validation Loss 0.068870\n",
            "Epoch 410, Loss 0.067101, Validation Loss 0.068908\n",
            "Epoch 411, Loss 0.067031, Validation Loss 0.069251\n",
            "Epoch 412, Loss 0.067049, Validation Loss 0.069109\n",
            "Epoch 413, Loss 0.066679, Validation Loss 0.068744\n",
            "Epoch 414, Loss 0.067008, Validation Loss 0.069333\n",
            "Epoch 415, Loss 0.066482, Validation Loss 0.069210\n",
            "Epoch 416, Loss 0.066570, Validation Loss 0.068872\n",
            "Epoch 417, Loss 0.066632, Validation Loss 0.068927\n",
            "Epoch 418, Loss 0.066485, Validation Loss 0.068383\n",
            "Epoch 419, Loss 0.066246, Validation Loss 0.068649\n",
            "Epoch 420, Loss 0.066382, Validation Loss 0.069672\n",
            "Epoch 421, Loss 0.066120, Validation Loss 0.068063\n",
            "Epoch 422, Loss 0.066099, Validation Loss 0.068499\n",
            "Epoch 423, Loss 0.065845, Validation Loss 0.068081\n",
            "Epoch 424, Loss 0.066075, Validation Loss 0.068786\n",
            "Epoch 425, Loss 0.065938, Validation Loss 0.068011\n",
            "Epoch 426, Loss 0.065915, Validation Loss 0.068302\n",
            "Epoch 427, Loss 0.065678, Validation Loss 0.068292\n",
            "Epoch 428, Loss 0.065638, Validation Loss 0.068218\n",
            "Epoch 429, Loss 0.065869, Validation Loss 0.068350\n",
            "Epoch 430, Loss 0.065750, Validation Loss 0.068074\n",
            "Epoch 431, Loss 0.065403, Validation Loss 0.067619\n",
            "Epoch 432, Loss 0.065483, Validation Loss 0.068225\n",
            "Epoch 433, Loss 0.065488, Validation Loss 0.067473\n",
            "Epoch 434, Loss 0.065337, Validation Loss 0.067259\n",
            "Epoch 435, Loss 0.065158, Validation Loss 0.066973\n",
            "Epoch 436, Loss 0.065483, Validation Loss 0.067336\n",
            "Epoch 437, Loss 0.065382, Validation Loss 0.067125\n",
            "Epoch 438, Loss 0.065067, Validation Loss 0.067410\n",
            "Epoch 439, Loss 0.065139, Validation Loss 0.067342\n",
            "Epoch 440, Loss 0.065205, Validation Loss 0.067342\n",
            "Epoch 441, Loss 0.064824, Validation Loss 0.067510\n",
            "Epoch 442, Loss 0.064927, Validation Loss 0.067027\n",
            "Epoch 443, Loss 0.064811, Validation Loss 0.067433\n",
            "Epoch 444, Loss 0.064852, Validation Loss 0.067116\n",
            "Epoch 445, Loss 0.064708, Validation Loss 0.067343\n",
            "Epoch 446, Loss 0.064463, Validation Loss 0.066762\n",
            "Epoch 447, Loss 0.064528, Validation Loss 0.066748\n",
            "Epoch 448, Loss 0.064934, Validation Loss 0.067054\n",
            "Epoch 449, Loss 0.064734, Validation Loss 0.067056\n",
            "Epoch 450, Loss 0.064120, Validation Loss 0.066597\n",
            "Epoch 451, Loss 0.064126, Validation Loss 0.066915\n",
            "Epoch 452, Loss 0.064151, Validation Loss 0.067063\n",
            "Epoch 453, Loss 0.064354, Validation Loss 0.066914\n",
            "Epoch 454, Loss 0.064285, Validation Loss 0.066492\n",
            "Epoch 455, Loss 0.064303, Validation Loss 0.066443\n",
            "Epoch 456, Loss 0.064310, Validation Loss 0.065964\n",
            "Epoch 457, Loss 0.064204, Validation Loss 0.066723\n",
            "Epoch 458, Loss 0.063809, Validation Loss 0.066255\n",
            "Epoch 459, Loss 0.063764, Validation Loss 0.066337\n",
            "Epoch 460, Loss 0.063834, Validation Loss 0.066496\n",
            "Epoch 461, Loss 0.064133, Validation Loss 0.066238\n",
            "Epoch 462, Loss 0.063803, Validation Loss 0.066096\n",
            "Epoch 463, Loss 0.063781, Validation Loss 0.066511\n",
            "Epoch 464, Loss 0.063433, Validation Loss 0.065989\n",
            "Epoch 465, Loss 0.063561, Validation Loss 0.066011\n",
            "Epoch 466, Loss 0.063427, Validation Loss 0.065941\n",
            "Epoch 467, Loss 0.063302, Validation Loss 0.065930\n",
            "Epoch 468, Loss 0.063540, Validation Loss 0.065725\n",
            "Epoch 469, Loss 0.063541, Validation Loss 0.065500\n",
            "Epoch 470, Loss 0.063463, Validation Loss 0.065456\n",
            "Epoch 471, Loss 0.063128, Validation Loss 0.065869\n",
            "Epoch 472, Loss 0.063200, Validation Loss 0.065977\n",
            "Epoch 473, Loss 0.063080, Validation Loss 0.065624\n",
            "Epoch 474, Loss 0.063058, Validation Loss 0.065612\n",
            "Epoch 475, Loss 0.063067, Validation Loss 0.065295\n",
            "Epoch 476, Loss 0.062972, Validation Loss 0.065515\n",
            "Epoch 477, Loss 0.062890, Validation Loss 0.065958\n",
            "Epoch 478, Loss 0.062691, Validation Loss 0.065416\n",
            "Epoch 479, Loss 0.062716, Validation Loss 0.065579\n",
            "Epoch 480, Loss 0.062547, Validation Loss 0.065519\n",
            "Epoch 481, Loss 0.062612, Validation Loss 0.065989\n",
            "Epoch 482, Loss 0.062575, Validation Loss 0.064976\n",
            "Epoch 483, Loss 0.062650, Validation Loss 0.064900\n",
            "Epoch 484, Loss 0.062384, Validation Loss 0.066067\n",
            "Epoch 485, Loss 0.062417, Validation Loss 0.064820\n",
            "Epoch 486, Loss 0.062478, Validation Loss 0.065388\n",
            "Epoch 487, Loss 0.062411, Validation Loss 0.064693\n",
            "Epoch 488, Loss 0.062061, Validation Loss 0.065073\n",
            "Epoch 489, Loss 0.062123, Validation Loss 0.065174\n",
            "Epoch 490, Loss 0.062428, Validation Loss 0.064604\n",
            "Epoch 491, Loss 0.062363, Validation Loss 0.065394\n",
            "Epoch 492, Loss 0.061951, Validation Loss 0.064665\n",
            "Epoch 493, Loss 0.062018, Validation Loss 0.063963\n",
            "Epoch 494, Loss 0.061867, Validation Loss 0.064673\n",
            "Epoch 495, Loss 0.061983, Validation Loss 0.064002\n",
            "Epoch 496, Loss 0.061734, Validation Loss 0.064700\n",
            "Epoch 497, Loss 0.061712, Validation Loss 0.064565\n",
            "Epoch 498, Loss 0.061512, Validation Loss 0.064412\n",
            "Epoch 499, Loss 0.061767, Validation Loss 0.064593\n",
            "Epoch 500, Loss 0.061666, Validation Loss 0.064161\n",
            "Epoch 501, Loss 0.061504, Validation Loss 0.064422\n",
            "Epoch 502, Loss 0.061746, Validation Loss 0.063959\n",
            "Epoch 503, Loss 0.061295, Validation Loss 0.064128\n",
            "Epoch 504, Loss 0.061332, Validation Loss 0.064290\n",
            "Epoch 505, Loss 0.061325, Validation Loss 0.064814\n",
            "Epoch 506, Loss 0.061551, Validation Loss 0.063997\n",
            "Epoch 507, Loss 0.061149, Validation Loss 0.063877\n",
            "Epoch 508, Loss 0.061334, Validation Loss 0.064255\n",
            "Epoch 509, Loss 0.061059, Validation Loss 0.063719\n",
            "Epoch 510, Loss 0.061166, Validation Loss 0.064051\n",
            "Epoch 511, Loss 0.061216, Validation Loss 0.064190\n",
            "Epoch 512, Loss 0.060911, Validation Loss 0.063298\n",
            "Epoch 513, Loss 0.061251, Validation Loss 0.063478\n",
            "Epoch 514, Loss 0.060837, Validation Loss 0.063735\n",
            "Epoch 515, Loss 0.060859, Validation Loss 0.064229\n",
            "Epoch 516, Loss 0.060851, Validation Loss 0.063375\n",
            "Epoch 517, Loss 0.060677, Validation Loss 0.063479\n",
            "Epoch 518, Loss 0.060554, Validation Loss 0.063196\n",
            "Epoch 519, Loss 0.060799, Validation Loss 0.063661\n",
            "Epoch 520, Loss 0.060777, Validation Loss 0.063419\n",
            "Epoch 521, Loss 0.060653, Validation Loss 0.063792\n",
            "Epoch 522, Loss 0.060818, Validation Loss 0.063092\n",
            "Epoch 523, Loss 0.060432, Validation Loss 0.063504\n",
            "Epoch 524, Loss 0.060378, Validation Loss 0.063550\n",
            "Epoch 525, Loss 0.060766, Validation Loss 0.063596\n",
            "Epoch 526, Loss 0.060487, Validation Loss 0.063423\n",
            "Epoch 527, Loss 0.060335, Validation Loss 0.063073\n",
            "Epoch 528, Loss 0.060198, Validation Loss 0.062952\n",
            "Epoch 529, Loss 0.060248, Validation Loss 0.063269\n",
            "Epoch 530, Loss 0.060005, Validation Loss 0.063381\n",
            "Epoch 531, Loss 0.059800, Validation Loss 0.062691\n",
            "Epoch 532, Loss 0.060022, Validation Loss 0.062476\n",
            "Epoch 533, Loss 0.060043, Validation Loss 0.063213\n",
            "Epoch 534, Loss 0.059811, Validation Loss 0.063082\n",
            "Epoch 535, Loss 0.059880, Validation Loss 0.062573\n",
            "Epoch 536, Loss 0.059930, Validation Loss 0.062849\n",
            "Epoch 537, Loss 0.059904, Validation Loss 0.062417\n",
            "Epoch 538, Loss 0.059514, Validation Loss 0.062411\n",
            "Epoch 539, Loss 0.059947, Validation Loss 0.062643\n",
            "Epoch 540, Loss 0.059253, Validation Loss 0.062938\n",
            "Epoch 541, Loss 0.059706, Validation Loss 0.062583\n",
            "Epoch 542, Loss 0.059513, Validation Loss 0.062613\n",
            "Epoch 543, Loss 0.059709, Validation Loss 0.062844\n",
            "Epoch 544, Loss 0.059188, Validation Loss 0.063056\n",
            "Epoch 545, Loss 0.059604, Validation Loss 0.062524\n",
            "Epoch 546, Loss 0.059345, Validation Loss 0.062458\n",
            "Epoch 547, Loss 0.059340, Validation Loss 0.062461\n",
            "Epoch 548, Loss 0.059407, Validation Loss 0.062074\n",
            "Epoch 549, Loss 0.059284, Validation Loss 0.062343\n",
            "Epoch 550, Loss 0.059625, Validation Loss 0.062261\n",
            "Epoch 551, Loss 0.059475, Validation Loss 0.062266\n",
            "Epoch 552, Loss 0.059322, Validation Loss 0.062105\n",
            "Epoch 553, Loss 0.059437, Validation Loss 0.062342\n",
            "Epoch 554, Loss 0.058903, Validation Loss 0.062558\n",
            "Epoch 555, Loss 0.059002, Validation Loss 0.062038\n",
            "Epoch 556, Loss 0.058995, Validation Loss 0.062039\n",
            "Epoch 557, Loss 0.058825, Validation Loss 0.061581\n",
            "Epoch 558, Loss 0.058983, Validation Loss 0.061405\n",
            "Epoch 559, Loss 0.058841, Validation Loss 0.062353\n",
            "Epoch 560, Loss 0.058683, Validation Loss 0.061282\n",
            "Epoch 561, Loss 0.058709, Validation Loss 0.061937\n",
            "Epoch 562, Loss 0.058877, Validation Loss 0.061984\n",
            "Epoch 563, Loss 0.058773, Validation Loss 0.061613\n",
            "Epoch 564, Loss 0.058479, Validation Loss 0.061761\n",
            "Epoch 565, Loss 0.058680, Validation Loss 0.061621\n",
            "Epoch 566, Loss 0.058371, Validation Loss 0.062051\n",
            "Epoch 567, Loss 0.058722, Validation Loss 0.061283\n",
            "Epoch 568, Loss 0.058432, Validation Loss 0.061494\n",
            "Epoch 569, Loss 0.058332, Validation Loss 0.062050\n",
            "Epoch 570, Loss 0.058088, Validation Loss 0.061071\n",
            "Epoch 571, Loss 0.058270, Validation Loss 0.061585\n",
            "Epoch 572, Loss 0.058233, Validation Loss 0.061244\n",
            "Epoch 573, Loss 0.058233, Validation Loss 0.061546\n",
            "Epoch 574, Loss 0.058304, Validation Loss 0.061675\n",
            "Epoch 575, Loss 0.058183, Validation Loss 0.061649\n",
            "Epoch 576, Loss 0.058031, Validation Loss 0.061748\n",
            "Epoch 577, Loss 0.058330, Validation Loss 0.061030\n",
            "Epoch 578, Loss 0.057746, Validation Loss 0.061166\n",
            "Epoch 579, Loss 0.057862, Validation Loss 0.061316\n",
            "Epoch 580, Loss 0.058185, Validation Loss 0.061451\n",
            "Epoch 581, Loss 0.058070, Validation Loss 0.061625\n",
            "Epoch 582, Loss 0.058146, Validation Loss 0.061474\n",
            "Epoch 583, Loss 0.057894, Validation Loss 0.061166\n",
            "Epoch 584, Loss 0.058016, Validation Loss 0.061071\n",
            "Epoch 585, Loss 0.057918, Validation Loss 0.060976\n",
            "Epoch 586, Loss 0.057721, Validation Loss 0.060965\n",
            "Epoch 587, Loss 0.057526, Validation Loss 0.060863\n",
            "Epoch 588, Loss 0.057846, Validation Loss 0.060868\n",
            "Epoch 589, Loss 0.057600, Validation Loss 0.060400\n",
            "Epoch 590, Loss 0.057672, Validation Loss 0.060702\n",
            "Epoch 591, Loss 0.057434, Validation Loss 0.060974\n",
            "Epoch 592, Loss 0.057449, Validation Loss 0.060564\n",
            "Epoch 593, Loss 0.057346, Validation Loss 0.061091\n",
            "Epoch 594, Loss 0.057486, Validation Loss 0.060693\n",
            "Epoch 595, Loss 0.057339, Validation Loss 0.060068\n",
            "Epoch 596, Loss 0.057440, Validation Loss 0.060964\n",
            "Epoch 597, Loss 0.057511, Validation Loss 0.060576\n",
            "Epoch 598, Loss 0.057061, Validation Loss 0.060627\n",
            "Epoch 599, Loss 0.057203, Validation Loss 0.060498\n",
            "Epoch 600, Loss 0.057159, Validation Loss 0.060524\n",
            "Epoch 601, Loss 0.057110, Validation Loss 0.059993\n",
            "Epoch 602, Loss 0.057058, Validation Loss 0.060655\n",
            "Epoch 603, Loss 0.057342, Validation Loss 0.060614\n",
            "Epoch 604, Loss 0.057217, Validation Loss 0.060706\n",
            "Epoch 605, Loss 0.057039, Validation Loss 0.060322\n",
            "Epoch 606, Loss 0.057050, Validation Loss 0.060455\n",
            "Epoch 607, Loss 0.056871, Validation Loss 0.060539\n",
            "Epoch 608, Loss 0.057114, Validation Loss 0.060105\n",
            "Epoch 609, Loss 0.057198, Validation Loss 0.060040\n",
            "Epoch 610, Loss 0.056826, Validation Loss 0.059599\n",
            "Epoch 611, Loss 0.056551, Validation Loss 0.059780\n",
            "Epoch 612, Loss 0.056674, Validation Loss 0.059895\n",
            "Epoch 613, Loss 0.056668, Validation Loss 0.059890\n",
            "Epoch 614, Loss 0.056718, Validation Loss 0.060008\n",
            "Epoch 615, Loss 0.056465, Validation Loss 0.059860\n",
            "Epoch 616, Loss 0.056518, Validation Loss 0.060443\n",
            "Epoch 617, Loss 0.056379, Validation Loss 0.060318\n",
            "Epoch 618, Loss 0.056367, Validation Loss 0.059036\n",
            "Epoch 619, Loss 0.056614, Validation Loss 0.060438\n",
            "Epoch 620, Loss 0.056315, Validation Loss 0.060538\n",
            "Epoch 621, Loss 0.056381, Validation Loss 0.059906\n",
            "Epoch 622, Loss 0.056373, Validation Loss 0.059502\n",
            "Epoch 623, Loss 0.056447, Validation Loss 0.059528\n",
            "Epoch 624, Loss 0.056433, Validation Loss 0.059442\n",
            "Epoch 625, Loss 0.056398, Validation Loss 0.059458\n",
            "Epoch 626, Loss 0.056189, Validation Loss 0.059414\n",
            "Epoch 627, Loss 0.056076, Validation Loss 0.059884\n",
            "Epoch 628, Loss 0.056246, Validation Loss 0.059277\n",
            "Epoch 629, Loss 0.055979, Validation Loss 0.059706\n",
            "Epoch 630, Loss 0.056127, Validation Loss 0.059679\n",
            "Epoch 631, Loss 0.056239, Validation Loss 0.059419\n",
            "Epoch 632, Loss 0.055756, Validation Loss 0.059437\n",
            "Epoch 633, Loss 0.055928, Validation Loss 0.059442\n",
            "Epoch 634, Loss 0.055894, Validation Loss 0.058888\n",
            "Epoch 635, Loss 0.056040, Validation Loss 0.059169\n",
            "Epoch 636, Loss 0.055744, Validation Loss 0.059452\n",
            "Epoch 637, Loss 0.055760, Validation Loss 0.059805\n",
            "Epoch 638, Loss 0.055824, Validation Loss 0.059571\n",
            "Epoch 639, Loss 0.055698, Validation Loss 0.059250\n",
            "Epoch 640, Loss 0.055557, Validation Loss 0.058970\n",
            "Epoch 641, Loss 0.055677, Validation Loss 0.059169\n",
            "Epoch 642, Loss 0.055674, Validation Loss 0.059163\n",
            "Epoch 643, Loss 0.055612, Validation Loss 0.059009\n",
            "Epoch 644, Loss 0.055528, Validation Loss 0.059505\n",
            "Epoch 645, Loss 0.055721, Validation Loss 0.058839\n",
            "Epoch 646, Loss 0.055620, Validation Loss 0.059229\n",
            "Epoch 647, Loss 0.055330, Validation Loss 0.059541\n",
            "Epoch 648, Loss 0.055275, Validation Loss 0.059327\n",
            "Epoch 649, Loss 0.055285, Validation Loss 0.059022\n",
            "Epoch 650, Loss 0.055272, Validation Loss 0.059322\n",
            "Epoch 651, Loss 0.055334, Validation Loss 0.059282\n",
            "Epoch 652, Loss 0.055347, Validation Loss 0.059299\n",
            "Epoch 653, Loss 0.055349, Validation Loss 0.058976\n",
            "Epoch 654, Loss 0.055232, Validation Loss 0.059365\n",
            "Epoch 655, Loss 0.055103, Validation Loss 0.058466\n",
            "Epoch 656, Loss 0.055306, Validation Loss 0.059307\n",
            "Epoch 657, Loss 0.055541, Validation Loss 0.058972\n",
            "Epoch 658, Loss 0.055017, Validation Loss 0.059255\n",
            "Epoch 659, Loss 0.055206, Validation Loss 0.058759\n",
            "Epoch 660, Loss 0.054883, Validation Loss 0.058784\n",
            "Epoch 661, Loss 0.054972, Validation Loss 0.058833\n",
            "Epoch 662, Loss 0.054868, Validation Loss 0.059139\n",
            "Epoch 663, Loss 0.054922, Validation Loss 0.058686\n",
            "Epoch 664, Loss 0.055014, Validation Loss 0.058461\n",
            "Epoch 665, Loss 0.055048, Validation Loss 0.058356\n",
            "Epoch 666, Loss 0.054768, Validation Loss 0.059274\n",
            "Epoch 667, Loss 0.054882, Validation Loss 0.058281\n",
            "Epoch 668, Loss 0.054909, Validation Loss 0.059244\n",
            "Epoch 669, Loss 0.054742, Validation Loss 0.057984\n",
            "Epoch 670, Loss 0.055017, Validation Loss 0.058411\n",
            "Epoch 671, Loss 0.054555, Validation Loss 0.058857\n",
            "Epoch 672, Loss 0.054515, Validation Loss 0.058244\n",
            "Epoch 673, Loss 0.054589, Validation Loss 0.058123\n",
            "Epoch 674, Loss 0.054555, Validation Loss 0.058469\n",
            "Epoch 675, Loss 0.054793, Validation Loss 0.058296\n",
            "Epoch 676, Loss 0.054570, Validation Loss 0.058358\n",
            "Epoch 677, Loss 0.054552, Validation Loss 0.058352\n",
            "Epoch 678, Loss 0.054633, Validation Loss 0.058418\n",
            "Epoch 679, Loss 0.054284, Validation Loss 0.058305\n",
            "Epoch 680, Loss 0.054293, Validation Loss 0.058656\n",
            "Epoch 681, Loss 0.054315, Validation Loss 0.058651\n",
            "Epoch 682, Loss 0.054482, Validation Loss 0.058343\n",
            "Epoch 683, Loss 0.054220, Validation Loss 0.058219\n",
            "Epoch 684, Loss 0.054632, Validation Loss 0.058583\n",
            "Epoch 685, Loss 0.054176, Validation Loss 0.058113\n",
            "Epoch 686, Loss 0.054310, Validation Loss 0.058268\n",
            "Epoch 687, Loss 0.054297, Validation Loss 0.058062\n",
            "Epoch 688, Loss 0.054267, Validation Loss 0.058038\n",
            "Epoch 689, Loss 0.054170, Validation Loss 0.058207\n",
            "Epoch 690, Loss 0.053996, Validation Loss 0.057248\n",
            "Epoch 691, Loss 0.054242, Validation Loss 0.058179\n",
            "Epoch 692, Loss 0.053944, Validation Loss 0.057926\n",
            "Epoch 693, Loss 0.054032, Validation Loss 0.058625\n",
            "Epoch 694, Loss 0.053823, Validation Loss 0.058327\n",
            "Epoch 695, Loss 0.054200, Validation Loss 0.058292\n",
            "Epoch 696, Loss 0.054126, Validation Loss 0.057506\n",
            "Epoch 697, Loss 0.053816, Validation Loss 0.057328\n",
            "Epoch 698, Loss 0.054183, Validation Loss 0.057867\n",
            "Epoch 699, Loss 0.053904, Validation Loss 0.057733\n",
            "Epoch 700, Loss 0.054005, Validation Loss 0.057556\n",
            "Epoch 701, Loss 0.054090, Validation Loss 0.057880\n",
            "Epoch 702, Loss 0.053798, Validation Loss 0.058087\n",
            "Epoch 703, Loss 0.053674, Validation Loss 0.058361\n",
            "Epoch 704, Loss 0.053847, Validation Loss 0.057946\n",
            "Epoch 705, Loss 0.053673, Validation Loss 0.057709\n",
            "Epoch 706, Loss 0.053419, Validation Loss 0.058359\n",
            "Epoch 707, Loss 0.053776, Validation Loss 0.058430\n",
            "Epoch 708, Loss 0.053800, Validation Loss 0.057303\n",
            "Epoch 709, Loss 0.053465, Validation Loss 0.058097\n",
            "Epoch 710, Loss 0.053771, Validation Loss 0.057175\n",
            "Epoch 711, Loss 0.053416, Validation Loss 0.057918\n",
            "Epoch 712, Loss 0.053490, Validation Loss 0.057362\n",
            "Epoch 713, Loss 0.053659, Validation Loss 0.057478\n",
            "Epoch 714, Loss 0.053629, Validation Loss 0.057312\n",
            "Epoch 715, Loss 0.053199, Validation Loss 0.057565\n",
            "Epoch 716, Loss 0.053424, Validation Loss 0.057846\n",
            "Epoch 717, Loss 0.053653, Validation Loss 0.057551\n",
            "Epoch 718, Loss 0.053515, Validation Loss 0.057799\n",
            "Epoch 719, Loss 0.053303, Validation Loss 0.056978\n",
            "Epoch 720, Loss 0.053339, Validation Loss 0.057518\n",
            "Epoch 721, Loss 0.053050, Validation Loss 0.057109\n",
            "Epoch 722, Loss 0.053278, Validation Loss 0.057193\n",
            "Epoch 723, Loss 0.053300, Validation Loss 0.057735\n",
            "Epoch 724, Loss 0.053145, Validation Loss 0.057691\n",
            "Epoch 725, Loss 0.053301, Validation Loss 0.057492\n",
            "Epoch 726, Loss 0.053035, Validation Loss 0.057457\n",
            "Epoch 727, Loss 0.053178, Validation Loss 0.057097\n",
            "Epoch 728, Loss 0.052845, Validation Loss 0.056911\n",
            "Epoch 729, Loss 0.053044, Validation Loss 0.057252\n",
            "Epoch 730, Loss 0.052876, Validation Loss 0.056877\n",
            "Epoch 731, Loss 0.053047, Validation Loss 0.057446\n",
            "Epoch 732, Loss 0.053177, Validation Loss 0.056820\n",
            "Epoch 733, Loss 0.052728, Validation Loss 0.056825\n",
            "Epoch 734, Loss 0.053296, Validation Loss 0.057182\n",
            "Epoch 735, Loss 0.052882, Validation Loss 0.056644\n",
            "Epoch 736, Loss 0.052941, Validation Loss 0.057118\n",
            "Epoch 737, Loss 0.052791, Validation Loss 0.057205\n",
            "Epoch 738, Loss 0.052608, Validation Loss 0.056600\n",
            "Epoch 739, Loss 0.052844, Validation Loss 0.056302\n",
            "Epoch 740, Loss 0.052862, Validation Loss 0.056667\n",
            "Epoch 741, Loss 0.052735, Validation Loss 0.056482\n",
            "Epoch 742, Loss 0.052727, Validation Loss 0.056654\n",
            "Epoch 743, Loss 0.052680, Validation Loss 0.056984\n",
            "Epoch 744, Loss 0.052548, Validation Loss 0.056947\n",
            "Epoch 745, Loss 0.052561, Validation Loss 0.056824\n",
            "Epoch 746, Loss 0.052540, Validation Loss 0.056746\n",
            "Epoch 747, Loss 0.052572, Validation Loss 0.056311\n",
            "Epoch 748, Loss 0.052754, Validation Loss 0.056433\n",
            "Epoch 749, Loss 0.052346, Validation Loss 0.057136\n",
            "Epoch 750, Loss 0.052443, Validation Loss 0.056120\n",
            "Epoch 751, Loss 0.052293, Validation Loss 0.056401\n",
            "Epoch 752, Loss 0.052744, Validation Loss 0.056549\n",
            "Epoch 753, Loss 0.052344, Validation Loss 0.056975\n",
            "Epoch 754, Loss 0.052498, Validation Loss 0.056627\n",
            "Epoch 755, Loss 0.052320, Validation Loss 0.056683\n",
            "Epoch 756, Loss 0.052274, Validation Loss 0.056562\n",
            "Epoch 757, Loss 0.052380, Validation Loss 0.056518\n",
            "Epoch 758, Loss 0.052498, Validation Loss 0.056202\n",
            "Epoch 759, Loss 0.052448, Validation Loss 0.056817\n",
            "Epoch 760, Loss 0.052159, Validation Loss 0.056292\n",
            "Epoch 761, Loss 0.052235, Validation Loss 0.056883\n",
            "Epoch 762, Loss 0.052031, Validation Loss 0.056634\n",
            "Epoch 763, Loss 0.052057, Validation Loss 0.056379\n",
            "Epoch 764, Loss 0.051990, Validation Loss 0.056022\n",
            "Epoch 765, Loss 0.052047, Validation Loss 0.056790\n",
            "Epoch 766, Loss 0.052064, Validation Loss 0.056723\n",
            "Epoch 767, Loss 0.052081, Validation Loss 0.056081\n",
            "Epoch 768, Loss 0.051867, Validation Loss 0.055949\n",
            "Epoch 769, Loss 0.052176, Validation Loss 0.056196\n",
            "Epoch 770, Loss 0.052293, Validation Loss 0.055851\n",
            "Epoch 771, Loss 0.052050, Validation Loss 0.056903\n",
            "Epoch 772, Loss 0.051945, Validation Loss 0.056795\n",
            "Epoch 773, Loss 0.052152, Validation Loss 0.056261\n",
            "Epoch 774, Loss 0.052049, Validation Loss 0.056562\n",
            "Epoch 775, Loss 0.051902, Validation Loss 0.056793\n",
            "Epoch 776, Loss 0.051463, Validation Loss 0.056044\n",
            "Epoch 777, Loss 0.052011, Validation Loss 0.055973\n",
            "Epoch 778, Loss 0.051738, Validation Loss 0.056737\n",
            "Epoch 779, Loss 0.051854, Validation Loss 0.056466\n",
            "Epoch 780, Loss 0.051751, Validation Loss 0.056236\n",
            "Epoch 781, Loss 0.051842, Validation Loss 0.055837\n",
            "Epoch 782, Loss 0.051858, Validation Loss 0.055444\n",
            "Epoch 783, Loss 0.051633, Validation Loss 0.056028\n",
            "Epoch 784, Loss 0.051894, Validation Loss 0.056037\n",
            "Epoch 785, Loss 0.051626, Validation Loss 0.055915\n",
            "Epoch 786, Loss 0.051641, Validation Loss 0.056299\n",
            "Epoch 787, Loss 0.051458, Validation Loss 0.055683\n",
            "Epoch 788, Loss 0.051529, Validation Loss 0.056473\n",
            "Epoch 789, Loss 0.051316, Validation Loss 0.056310\n",
            "Epoch 790, Loss 0.051380, Validation Loss 0.055889\n",
            "Epoch 791, Loss 0.051561, Validation Loss 0.056030\n",
            "Epoch 792, Loss 0.051574, Validation Loss 0.055408\n",
            "Epoch 793, Loss 0.051531, Validation Loss 0.056283\n",
            "Epoch 794, Loss 0.051248, Validation Loss 0.055506\n",
            "Epoch 795, Loss 0.051582, Validation Loss 0.056209\n",
            "Epoch 796, Loss 0.051174, Validation Loss 0.055761\n",
            "Epoch 797, Loss 0.051544, Validation Loss 0.056250\n",
            "Epoch 798, Loss 0.051398, Validation Loss 0.055957\n",
            "Epoch 799, Loss 0.051502, Validation Loss 0.055786\n",
            "Epoch 800, Loss 0.051107, Validation Loss 0.056102\n",
            "Epoch 801, Loss 0.051247, Validation Loss 0.055914\n",
            "Epoch 802, Loss 0.051178, Validation Loss 0.055900\n",
            "Epoch 803, Loss 0.050912, Validation Loss 0.055377\n",
            "Epoch 804, Loss 0.051078, Validation Loss 0.055740\n",
            "Epoch 805, Loss 0.051182, Validation Loss 0.055722\n",
            "Epoch 806, Loss 0.051110, Validation Loss 0.056023\n",
            "Epoch 807, Loss 0.050937, Validation Loss 0.055546\n",
            "Epoch 808, Loss 0.051113, Validation Loss 0.056007\n",
            "Epoch 809, Loss 0.050896, Validation Loss 0.055517\n",
            "Epoch 810, Loss 0.051040, Validation Loss 0.056013\n",
            "Epoch 811, Loss 0.051226, Validation Loss 0.055495\n",
            "Epoch 812, Loss 0.051160, Validation Loss 0.055948\n",
            "Epoch 813, Loss 0.051039, Validation Loss 0.055837\n",
            "Epoch 814, Loss 0.051015, Validation Loss 0.055261\n",
            "Epoch 815, Loss 0.050895, Validation Loss 0.055575\n",
            "Epoch 816, Loss 0.051004, Validation Loss 0.055783\n",
            "Epoch 817, Loss 0.050687, Validation Loss 0.055526\n",
            "Epoch 818, Loss 0.050708, Validation Loss 0.055603\n",
            "Epoch 819, Loss 0.050950, Validation Loss 0.055745\n",
            "Epoch 820, Loss 0.050697, Validation Loss 0.055227\n",
            "Epoch 821, Loss 0.050780, Validation Loss 0.055748\n",
            "Epoch 822, Loss 0.050737, Validation Loss 0.055400\n",
            "Epoch 823, Loss 0.050741, Validation Loss 0.055573\n",
            "Epoch 824, Loss 0.050655, Validation Loss 0.055696\n",
            "Epoch 825, Loss 0.050945, Validation Loss 0.055392\n",
            "Epoch 826, Loss 0.050602, Validation Loss 0.055278\n",
            "Epoch 827, Loss 0.050497, Validation Loss 0.055064\n",
            "Epoch 828, Loss 0.050570, Validation Loss 0.055575\n",
            "Epoch 829, Loss 0.050472, Validation Loss 0.055123\n",
            "Epoch 830, Loss 0.050654, Validation Loss 0.054992\n",
            "Epoch 831, Loss 0.050528, Validation Loss 0.055040\n",
            "Epoch 832, Loss 0.050660, Validation Loss 0.055244\n",
            "Epoch 833, Loss 0.050239, Validation Loss 0.055064\n",
            "Epoch 834, Loss 0.050494, Validation Loss 0.055215\n",
            "Epoch 835, Loss 0.050347, Validation Loss 0.055387\n",
            "Epoch 836, Loss 0.050293, Validation Loss 0.055570\n",
            "Epoch 837, Loss 0.050634, Validation Loss 0.055229\n",
            "Epoch 838, Loss 0.050406, Validation Loss 0.055549\n",
            "Epoch 839, Loss 0.050446, Validation Loss 0.055587\n",
            "Epoch 840, Loss 0.050339, Validation Loss 0.055756\n",
            "Epoch 841, Loss 0.050168, Validation Loss 0.055131\n",
            "Epoch 842, Loss 0.050487, Validation Loss 0.055157\n",
            "Epoch 843, Loss 0.050125, Validation Loss 0.055330\n",
            "Epoch 844, Loss 0.050371, Validation Loss 0.055279\n",
            "Epoch 845, Loss 0.050174, Validation Loss 0.054912\n",
            "Epoch 846, Loss 0.050143, Validation Loss 0.054782\n",
            "Epoch 847, Loss 0.050001, Validation Loss 0.054904\n",
            "Epoch 848, Loss 0.050467, Validation Loss 0.054869\n",
            "Epoch 849, Loss 0.050118, Validation Loss 0.055333\n",
            "Epoch 850, Loss 0.050065, Validation Loss 0.054860\n",
            "Epoch 851, Loss 0.049900, Validation Loss 0.055000\n",
            "Epoch 852, Loss 0.050032, Validation Loss 0.055289\n",
            "Epoch 853, Loss 0.050180, Validation Loss 0.054612\n",
            "Epoch 854, Loss 0.050089, Validation Loss 0.055344\n",
            "Epoch 855, Loss 0.049768, Validation Loss 0.054981\n",
            "Epoch 856, Loss 0.050278, Validation Loss 0.054690\n",
            "Epoch 857, Loss 0.049992, Validation Loss 0.054897\n",
            "Epoch 858, Loss 0.049850, Validation Loss 0.054222\n",
            "Epoch 859, Loss 0.050087, Validation Loss 0.055080\n",
            "Epoch 860, Loss 0.050005, Validation Loss 0.055267\n",
            "Epoch 861, Loss 0.049912, Validation Loss 0.054635\n",
            "Epoch 862, Loss 0.049911, Validation Loss 0.054887\n",
            "Epoch 863, Loss 0.049990, Validation Loss 0.054803\n",
            "Epoch 864, Loss 0.049895, Validation Loss 0.054694\n",
            "Epoch 865, Loss 0.049637, Validation Loss 0.054523\n",
            "Epoch 866, Loss 0.049826, Validation Loss 0.054367\n",
            "Epoch 867, Loss 0.049529, Validation Loss 0.054602\n",
            "Epoch 868, Loss 0.049927, Validation Loss 0.054606\n",
            "Epoch 869, Loss 0.049753, Validation Loss 0.054987\n",
            "Epoch 870, Loss 0.049821, Validation Loss 0.054367\n",
            "Epoch 871, Loss 0.049499, Validation Loss 0.054637\n",
            "Epoch 872, Loss 0.049596, Validation Loss 0.054636\n",
            "Epoch 873, Loss 0.049841, Validation Loss 0.054833\n",
            "Epoch 874, Loss 0.049681, Validation Loss 0.054738\n",
            "Epoch 875, Loss 0.049769, Validation Loss 0.053877\n",
            "Epoch 876, Loss 0.049713, Validation Loss 0.054423\n",
            "Epoch 877, Loss 0.049437, Validation Loss 0.054475\n",
            "Epoch 878, Loss 0.049725, Validation Loss 0.054598\n",
            "Epoch 879, Loss 0.049587, Validation Loss 0.054046\n",
            "Epoch 880, Loss 0.049351, Validation Loss 0.054366\n",
            "Epoch 881, Loss 0.049554, Validation Loss 0.054566\n",
            "Epoch 882, Loss 0.049649, Validation Loss 0.054220\n",
            "Epoch 883, Loss 0.049307, Validation Loss 0.054286\n",
            "Epoch 884, Loss 0.049192, Validation Loss 0.054147\n",
            "Epoch 885, Loss 0.049580, Validation Loss 0.055145\n",
            "Epoch 886, Loss 0.049233, Validation Loss 0.054292\n",
            "Epoch 887, Loss 0.049328, Validation Loss 0.054552\n",
            "Epoch 888, Loss 0.049182, Validation Loss 0.054145\n",
            "Epoch 889, Loss 0.049148, Validation Loss 0.054408\n",
            "Epoch 890, Loss 0.049371, Validation Loss 0.054564\n",
            "Epoch 891, Loss 0.049130, Validation Loss 0.054453\n",
            "Epoch 892, Loss 0.049156, Validation Loss 0.054912\n",
            "Epoch 893, Loss 0.049332, Validation Loss 0.053929\n",
            "Epoch 894, Loss 0.049291, Validation Loss 0.053832\n",
            "Epoch 895, Loss 0.049241, Validation Loss 0.054102\n",
            "Epoch 896, Loss 0.049569, Validation Loss 0.054675\n",
            "Epoch 897, Loss 0.049283, Validation Loss 0.054521\n",
            "Epoch 898, Loss 0.049142, Validation Loss 0.054259\n",
            "Epoch 899, Loss 0.048992, Validation Loss 0.053547\n",
            "Epoch 900, Loss 0.048908, Validation Loss 0.054788\n",
            "Epoch 901, Loss 0.049074, Validation Loss 0.054423\n",
            "Epoch 902, Loss 0.049030, Validation Loss 0.054687\n",
            "Epoch 903, Loss 0.048802, Validation Loss 0.054317\n",
            "Epoch 904, Loss 0.049039, Validation Loss 0.053543\n",
            "Epoch 905, Loss 0.049014, Validation Loss 0.054955\n",
            "Epoch 906, Loss 0.048997, Validation Loss 0.054565\n",
            "Epoch 907, Loss 0.048967, Validation Loss 0.054214\n",
            "Epoch 908, Loss 0.049112, Validation Loss 0.054516\n",
            "Epoch 909, Loss 0.048869, Validation Loss 0.053924\n",
            "Epoch 910, Loss 0.049071, Validation Loss 0.054225\n",
            "Epoch 911, Loss 0.048903, Validation Loss 0.054394\n",
            "Epoch 912, Loss 0.048817, Validation Loss 0.053790\n",
            "Epoch 913, Loss 0.048718, Validation Loss 0.054364\n",
            "Epoch 914, Loss 0.048861, Validation Loss 0.054008\n",
            "Epoch 915, Loss 0.048802, Validation Loss 0.054161\n",
            "Epoch 916, Loss 0.048812, Validation Loss 0.053519\n",
            "Epoch 917, Loss 0.048717, Validation Loss 0.054209\n",
            "Epoch 918, Loss 0.048820, Validation Loss 0.053781\n",
            "Epoch 919, Loss 0.048719, Validation Loss 0.054260\n",
            "Epoch 920, Loss 0.048733, Validation Loss 0.054498\n",
            "Epoch 921, Loss 0.048558, Validation Loss 0.053833\n",
            "Epoch 922, Loss 0.048569, Validation Loss 0.053822\n",
            "Epoch 923, Loss 0.048548, Validation Loss 0.053870\n",
            "Epoch 924, Loss 0.048715, Validation Loss 0.054053\n",
            "Epoch 925, Loss 0.048892, Validation Loss 0.053622\n",
            "Epoch 926, Loss 0.048475, Validation Loss 0.054243\n",
            "Epoch 927, Loss 0.048353, Validation Loss 0.053776\n",
            "Epoch 928, Loss 0.048492, Validation Loss 0.054048\n",
            "Epoch 929, Loss 0.048419, Validation Loss 0.053254\n",
            "Epoch 930, Loss 0.048553, Validation Loss 0.053784\n",
            "Epoch 931, Loss 0.048540, Validation Loss 0.053773\n",
            "Epoch 932, Loss 0.048508, Validation Loss 0.053558\n",
            "Epoch 933, Loss 0.048522, Validation Loss 0.053362\n",
            "Epoch 934, Loss 0.048281, Validation Loss 0.054088\n",
            "Epoch 935, Loss 0.048408, Validation Loss 0.053688\n",
            "Epoch 936, Loss 0.048065, Validation Loss 0.053574\n",
            "Epoch 937, Loss 0.048367, Validation Loss 0.054082\n",
            "Epoch 938, Loss 0.048411, Validation Loss 0.053325\n",
            "Epoch 939, Loss 0.048481, Validation Loss 0.053512\n",
            "Epoch 940, Loss 0.048319, Validation Loss 0.053759\n",
            "Epoch 941, Loss 0.048318, Validation Loss 0.054020\n",
            "Epoch 942, Loss 0.048017, Validation Loss 0.053297\n",
            "Epoch 943, Loss 0.048344, Validation Loss 0.053816\n",
            "Epoch 944, Loss 0.048096, Validation Loss 0.053932\n",
            "Epoch 945, Loss 0.048107, Validation Loss 0.053723\n",
            "Epoch 946, Loss 0.048478, Validation Loss 0.053687\n",
            "Epoch 947, Loss 0.048174, Validation Loss 0.053803\n",
            "Epoch 948, Loss 0.048252, Validation Loss 0.053930\n",
            "Epoch 949, Loss 0.048412, Validation Loss 0.053950\n",
            "Epoch 950, Loss 0.048230, Validation Loss 0.053703\n",
            "Epoch 951, Loss 0.048042, Validation Loss 0.053312\n",
            "Epoch 952, Loss 0.048160, Validation Loss 0.053965\n",
            "Epoch 953, Loss 0.048291, Validation Loss 0.053789\n",
            "Epoch 954, Loss 0.048355, Validation Loss 0.053452\n",
            "Epoch 955, Loss 0.048179, Validation Loss 0.053384\n",
            "Epoch 956, Loss 0.047688, Validation Loss 0.053449\n",
            "Epoch 957, Loss 0.048007, Validation Loss 0.053882\n",
            "Epoch 958, Loss 0.048132, Validation Loss 0.053634\n",
            "Epoch 959, Loss 0.048062, Validation Loss 0.053071\n",
            "Epoch 960, Loss 0.048042, Validation Loss 0.053318\n",
            "Epoch 961, Loss 0.048159, Validation Loss 0.053501\n",
            "Epoch 962, Loss 0.047787, Validation Loss 0.053416\n",
            "Epoch 963, Loss 0.047765, Validation Loss 0.053133\n",
            "Epoch 964, Loss 0.048117, Validation Loss 0.053521\n",
            "Epoch 965, Loss 0.047772, Validation Loss 0.053214\n",
            "Epoch 966, Loss 0.047561, Validation Loss 0.053114\n",
            "Epoch 967, Loss 0.047792, Validation Loss 0.053164\n",
            "Epoch 968, Loss 0.047873, Validation Loss 0.053407\n",
            "Epoch 969, Loss 0.047875, Validation Loss 0.053191\n",
            "Epoch 970, Loss 0.047666, Validation Loss 0.053312\n",
            "Epoch 971, Loss 0.047740, Validation Loss 0.053212\n",
            "Epoch 972, Loss 0.047720, Validation Loss 0.053814\n",
            "Epoch 973, Loss 0.047622, Validation Loss 0.053588\n",
            "Epoch 974, Loss 0.047703, Validation Loss 0.053005\n",
            "Epoch 975, Loss 0.047799, Validation Loss 0.052870\n",
            "Epoch 976, Loss 0.047754, Validation Loss 0.053046\n",
            "Epoch 977, Loss 0.047461, Validation Loss 0.053195\n",
            "Epoch 978, Loss 0.047855, Validation Loss 0.052547\n",
            "Epoch 979, Loss 0.047485, Validation Loss 0.053298\n",
            "Epoch 980, Loss 0.047697, Validation Loss 0.053015\n",
            "Epoch 981, Loss 0.047221, Validation Loss 0.053426\n",
            "Epoch 982, Loss 0.047608, Validation Loss 0.052737\n",
            "Epoch 983, Loss 0.047706, Validation Loss 0.053667\n",
            "Epoch 984, Loss 0.047448, Validation Loss 0.053656\n",
            "Epoch 985, Loss 0.047404, Validation Loss 0.053023\n",
            "Epoch 986, Loss 0.047346, Validation Loss 0.053007\n",
            "Epoch 987, Loss 0.047247, Validation Loss 0.052964\n",
            "Epoch 988, Loss 0.047395, Validation Loss 0.053400\n",
            "Epoch 989, Loss 0.047342, Validation Loss 0.053459\n",
            "Epoch 990, Loss 0.047396, Validation Loss 0.052890\n",
            "Epoch 991, Loss 0.047362, Validation Loss 0.052980\n",
            "Epoch 992, Loss 0.047347, Validation Loss 0.053011\n",
            "Epoch 993, Loss 0.047587, Validation Loss 0.052717\n",
            "Epoch 994, Loss 0.047297, Validation Loss 0.052770\n",
            "Epoch 995, Loss 0.047455, Validation Loss 0.052714\n",
            "Epoch 996, Loss 0.047353, Validation Loss 0.053015\n",
            "Epoch 997, Loss 0.047198, Validation Loss 0.052954\n",
            "Epoch 998, Loss 0.047088, Validation Loss 0.053001\n",
            "Epoch 999, Loss 0.047299, Validation Loss 0.052813\n",
            "Neural Network Parameter Count: 69650\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "Xtrain, Xval, Ttrain, Tval = train_test_split(dfX, dfT, train_size = 0.8, test_size=0.2)\n",
        "\n",
        "class AV_NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, weight_decay = 0.0001, dropout_prob=0.1):\n",
        "        super(AV_NN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        input_dim = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            self.hidden_layers.append(nn.Linear(input_dim, hidden_size))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "            self.hidden_layers.append(nn.Dropout(p=dropout_prob))\n",
        "            input_dim = hidden_size\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "        # Add L2 regularization to all linear layers\n",
        "        self.l2_regularizer = nn.Linear(hidden_sizes[-1], 1)\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        l2_reg = 0.0\n",
        "        for param in self.parameters():\n",
        "            l2_reg += torch.sum(param ** 2)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "XtrainT = torch.tensor(Xtrain.values, dtype=torch.float)\n",
        "TtrainT = torch.tensor(Ttrain, dtype=torch.float)\n",
        "XvalT = torch.tensor(Xval.values, dtype=torch.float)\n",
        "TvalT = torch.tensor(Tval, dtype=torch.float)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "model = AV_NN(Xtrain.shape[1], [160, 320, 16], 1, dropout_prob=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
        "lossFn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vpredictions = model(XvalT)\n",
        "    vloss = lossFn(vpredictions, TvalT.view(-1, 1))\n",
        "\n",
        "    predictions = model(XtrainT)\n",
        "    loss = lossFn(predictions, TtrainT.view(-1, 1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "      print('Epoch %d, Loss %f, Validation Loss %f' % (epoch, float(loss), float(vloss)))\n",
        "\n",
        "nn_complexity = sum(p.numel() for p in model.parameters())\n",
        "print(f'Neural Network Parameter Count: {nn_complexity}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "udf = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/4105av/unseen.csv')\n",
        "\n",
        "# threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()\n",
        "print(XvalT)\n",
        "with torch.no_grad():\n",
        "    vpredictions = model(XvalT)\n",
        "\n",
        "# Convert the predictions to binary values (0 or 1)\n",
        "v_predictions_binary = np.where(vpredictions > threshold, 1, 0)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(TvalT.cpu().numpy(), v_predictions_binary)\n",
        "precision = precision_score(TvalT.cpu().numpy(), v_predictions_binary)\n",
        "recall = recall_score(TvalT.cpu().numpy(), v_predictions_binary)\n",
        "f1 = f1_score(TvalT.cpu().numpy(), v_predictions_binary)\n",
        "confusion = confusion_matrix(TvalT.cpu().numpy(), v_predictions_binary)\n",
        "print(confusion)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Neural network complexity\n",
        "nn_complexity = sum(p.numel() for p in model.parameters())\n",
        "print(f'Neural Network Parameter Count: {nn_complexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzcKq6NtFwQt",
        "outputId": "f1c16583-a3c7-4949-85b0-501e699666a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 2.5000e-01, 6.9442e-01,\n",
            "         4.8917e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 2.5000e-01, 9.8895e-01,\n",
            "         2.0398e-02],\n",
            "        [0.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.3333e-01, 7.4150e-01,\n",
            "         4.8005e-04],\n",
            "        ...,\n",
            "        [1.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 2.0000e-01, 8.7915e-01,\n",
            "         4.7422e-03],\n",
            "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.6667e-01, 5.7299e-01,\n",
            "         1.0482e-02],\n",
            "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 4.0000e-01, 5.2141e-01,\n",
            "         2.4120e-03]])\n",
            "[[16096  1057]\n",
            " [  937 22220]]\n",
            "Accuracy: 0.9505\n",
            "Precision: 0.9546\n",
            "Recall: 0.9595\n",
            "F1 Score: 0.9571\n",
            "Neural Network Parameter Count: 69650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Dataset\n",
        "testSet = torch.tensor(udf.values, dtype=torch.float)\n",
        "\n",
        "with torch.no_grad():\n",
        "  upredictions = model(testSet)\n",
        "\n",
        "print(upredictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9ccV-OOvBkr",
        "outputId": "c393ab70-8bd1-4009-d871-639a9aa9bdaa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0402],\n",
            "        [0.0452],\n",
            "        [0.7297],\n",
            "        [0.0327],\n",
            "        [0.0067],\n",
            "        [0.0016],\n",
            "        [0.9665],\n",
            "        [0.9244],\n",
            "        [0.8043],\n",
            "        [0.0790],\n",
            "        [0.9641],\n",
            "        [0.8187],\n",
            "        [0.0342]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}